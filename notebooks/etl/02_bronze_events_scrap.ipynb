{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPO0cGlfAQbPveEof/bonmp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucasreis95/world-surf-league-data/blob/main/notebooks/etl/02_bronze_events_scrap.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyD4kQwVpyZE"
      },
      "outputs": [],
      "source": [
        "# import libs\n",
        "import pandas as pd\n",
        "from datetime import date\n",
        "import pandas_gbq\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get main url (don't include year)\n",
        "url = 'https://www.worldsurfleague.com/athletes/tour/mct?year='"
      ],
      "metadata": {
        "id": "DXZVg4_ZqY4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set empty dfs that will be filled\n",
        "year_list_to_df = []\n",
        "event_name_list = []\n",
        "event_order_list = []\n",
        "# loop throught years to get all events\n",
        "year_list = [i for i in range(2010, 2025) if i != 2020]\n",
        "for year in year_list:\n",
        "  # convert to string to concatenate with url\n",
        "  year_str = str(year)\n",
        "  # make request\n",
        "  r = requests.get(url + year_str).text\n",
        "  soup = BeautifulSoup(r, 'html.parser')\n",
        "  # get html with all events of a year\n",
        "  all_events_html = soup.find_all('span', attrs = {'class':'tooltip-item'})\n",
        "  # we use a way greater range than the maximum events per year\n",
        "  # to guarantee that we will include all events\n",
        "  for i in range(18):\n",
        "    try:\n",
        "      # get event name\n",
        "      event_name = all_events_html[i]['data-tooltip'].split('\\\\\">')[2].split('<\\\\/')[0]\n",
        "      # append auxiliar information to empty lists\n",
        "      event_name_list.append(event_name)\n",
        "      event_order_list.append(i)\n",
        "      year_list_to_df.append(year)\n",
        "    # as we don't have a fixed number of events per year, we need to use this exception\n",
        "    # and element 'data-tooltip' is not provided for future events\n",
        "    except:\n",
        "      pass\n",
        "    try:\n",
        "      # get event name\n",
        "      event_name = all_events_html[i]['data-tooltip'].split('\\\\\">')[1].split('<\\\\/')[0] # 2023 and 2024 rankings are slight different\n",
        "      # append auxiliar information to empty lists\n",
        "      event_name_list.append(event_name)\n",
        "      event_order_list.append(i)\n",
        "      year_list_to_df.append(year)\n",
        "    except:\n",
        "      pass\n",
        "  print('Events from ', year, ' has been colected.')\n",
        "\n",
        "# create df\n",
        "df_events = pd.DataFrame(\n",
        "                        {'season_year': year_list_to_df,\n",
        "                         'event_order': event_order_list,\n",
        "                         'event_name': event_name_list\n",
        "                        })\n",
        "\n",
        "#remove elemetns from df\n",
        "df_events = df_events[~df_events['event_name'].str.contains('<span')]\n",
        "# sum 1 to event_order to start from 1 instead of 0\n",
        "df_events['event_order'] = df_events['event_order'] + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4u_cGnhqVu7",
        "outputId": "74d72f2a-c5b9-4e86-9698-3f7a697ee1ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Events from  2010  has been colected.\n",
            "Events from  2011  has been colected.\n",
            "Events from  2012  has been colected.\n",
            "Events from  2013  has been colected.\n",
            "Events from  2014  has been colected.\n",
            "Events from  2015  has been colected.\n",
            "Events from  2016  has been colected.\n",
            "Events from  2017  has been colected.\n",
            "Events from  2018  has been colected.\n",
            "Events from  2019  has been colected.\n",
            "Events from  2021  has been colected.\n",
            "Events from  2022  has been colected.\n",
            "Events from  2023  has been colected.\n",
            "Events from  2024  has been colected.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# write raw table in big query\n",
        "pandas_gbq.to_gbq(\n",
        "                  dataframe = df_events,\n",
        "                  destination_table = 'wsl-data-397017.01_bronze.wsl_events_scrap',\n",
        "                  project_id = 'wsl-data-397017',\n",
        "                  if_exists = 'replace'\n",
        "                  )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qe8GyFU4qgBQ",
        "outputId": "781b5664-5581-47af-a4e9-d5c889e190f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 7943.76it/s]\n"
          ]
        }
      ]
    }
  ]
}